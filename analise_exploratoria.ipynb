{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando libs para análise\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ato Declaratorio Interpretativo SRF nº 15, de ...</td>\n",
       "      <td>Ato Declaratório Interpretativo SRF nº 15, de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ato Declaratorio nº 01, de 2013</td>\n",
       "      <td>* Publicado no DOE em 01/02/2013 ATO DECLARA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ato Declaratorio nº 02, de 2013</td>\n",
       "      <td>* Publicado no DOE em 01/02/2013 ATO DECLARA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ato Declaratório nº 01, de 1998</td>\n",
       "      <td>ATO DECLARATÓRIO Nº 01/1998 07/01/1998 * Publi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ato Declaratório nº 01, de 1999</td>\n",
       "      <td>ATO DECLARATÓRIO Nº 01/1999 14/01/1999 * Publi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Ato Declaratorio Interpretativo SRF nº 15, de ...   \n",
       "1                    Ato Declaratorio nº 01, de 2013   \n",
       "2                    Ato Declaratorio nº 02, de 2013   \n",
       "3                    Ato Declaratório nº 01, de 1998   \n",
       "4                    Ato Declaratório nº 01, de 1999   \n",
       "\n",
       "                                             content  \n",
       "0  Ato Declaratório Interpretativo SRF nº 15, de ...  \n",
       "1    * Publicado no DOE em 01/02/2013 ATO DECLARA...  \n",
       "2    * Publicado no DOE em 01/02/2013 ATO DECLARA...  \n",
       "3  ATO DECLARATÓRIO Nº 01/1998 07/01/1998 * Publi...  \n",
       "4  ATO DECLARATÓRIO Nº 01/1999 14/01/1999 * Publi...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtendo dados\n",
    "df = pd.read_csv('dataset.csv')\n",
    "df.columns = df.columns.str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3606"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tamanho do dataset\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificando por dados faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title      0\n",
       "content    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando por dados faltantes\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3604"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removendo dados faltantes\n",
    "df.dropna(inplace=True)\n",
    "# Novo tamanho do dataset\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificando por ambiguidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tipos_de_documentos = [\"ato declaratório\", \"ato declaratorio\", \"decreto\", \"instrução normativa\", \n",
    "                      \"lei\", \"lei complementar\", \"norma de execução\", \"nota explicativa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Ato Declaratorio Interpretativo SRF nº 15, de ...\n",
       "1                      Ato Declaratorio nº 01, de 2013\n",
       "2                      Ato Declaratorio nº 02, de 2013\n",
       "3                      Ato Declaratório nº 01, de 1998\n",
       "4                      Ato Declaratório nº 01, de 1999\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_ocorrencias(documento):\n",
    "    \"\"\"Conta o número de ocorrências para cada tipo de classe de documento\"\"\"\n",
    "    return df['title'].apply(lambda t: bool(re.match(documento, t.lower()))).sum()\n",
    "\n",
    "def indices_ocorrencias(documento):\n",
    "    \"\"\"Gera uma lista de índices para cada ocorrência de tipo de classe de documento\"\"\"\n",
    "    return df.index[df['title'].apply(lambda t: bool(re.match(documento, t.lower())))].tolist()\n",
    "\n",
    "\n",
    "dist_documentos = [num_ocorrencias(documento) for documento in tipos_de_documentos]\n",
    "indices_documentos = [indices_ocorrencias(documento) for documento in tipos_de_documentos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuição das classes:\n",
      "\n",
      "0 ato declaratório 669\n",
      "1 ato declaratorio 3\n",
      "2 decreto 1024\n",
      "3 instrução normativa 1376\n",
      "4 lei 331\n",
      "5 lei complementar 72\n",
      "6 norma de execução 117\n",
      "7 nota explicativa 84\n",
      "\n",
      "Total: 3676\n"
     ]
    }
   ],
   "source": [
    "indice = 0\n",
    "\n",
    "print(\"Distribuição das classes:\\n\")\n",
    "\n",
    "for documento, qtd in zip(tipos_de_documentos, dist_documentos):\n",
    "    print(indice, documento, qtd)\n",
    "    indice += 1\n",
    "\n",
    "print(\"\\nTotal:\", sum(dist_documentos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aparentemente, as classes 0 e 1 pertencem ao mesmo grupo (*ato declaratório*). Vamos defini-las como uma só:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Ato Declaratório Interpretativo SRF nº 15, de ...\n",
       "1                      Ato Declaratório nº 01, de 2013\n",
       "2                      Ato Declaratório nº 02, de 2013\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Índices das tuplas com título \"ato declaratorio\"\n",
    "print(indices_documentos[1])\n",
    "\n",
    "for i in range(3):\n",
    "    df['title'].iloc[i] = df['title'].iloc[i].replace('Declaratorio', 'Declaratório')\n",
    "    \n",
    "df['title'].iloc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Há alguma diferença no número de exemplares de classes total e o número total de dados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3604, 3676)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0], sum(dist_documentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 4 j: 5\n",
      "i: 5 j: 4\n"
     ]
    }
   ],
   "source": [
    "conjs_indices_documentos = [set(lista) for lista in indices_documentos]\n",
    "\n",
    "# Observando se há interseções entre os índices dois a dois (i.e. se há dados com duas classes)\n",
    "for i in range(0, 8):\n",
    "    for j in range(0, 8):\n",
    "        if i != j and set.intersection(conjs_indices_documentos[i], conjs_indices_documentos[j]):\n",
    "            print(f\"i: {i} j: {j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3077    Lei Complementar 0024, de 1975 (Cópia em Traba...\n",
       "3078    Lei Complementar 0024, de 1975 (Cópia em Traba...\n",
       "3079    Lei Complementar 0024, de 1975 (Cópia em Traba...\n",
       "3080    Lei Complementar 0024, de 1975 (Cópia em Traba...\n",
       "3081    Lei Complementar 0024, de 1975 (Cópia em Traba...\n",
       "                              ...                        \n",
       "3144                      Lei Complementar nº 50, de 2004\n",
       "3145                      Lei Complementar nº 81, de 2009\n",
       "3146                      Lei Complementar nº 81, de 2009\n",
       "3147                               Lei n° 08.137, de 1990\n",
       "3148                               Lei n° 08.397, de 1992\n",
       "Name: title, Length: 72, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersect_lista = [el for el in set.intersection(conjs_indices_documentos[4], conjs_indices_documentos[5])]\n",
    "df['title'].iloc[df.index[intersect_lista]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados supostamente ambíguos referem-se a títulos com a palavra \"lei\", mas definimos duas classes diferentes, i.e., \"lei\" e \"lei complementar\". Uma vez que a palavra *lei* é encontrada em ambos, não há problemas. Como a classe é contada duas vezes, retiremos a diferença:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3604, 3604)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(dist_documentos) - len(intersect_lista), df.shape[0] # (Diferença, Tamanho real do dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aparentemente, não há ambiguidade nos títulos. Podemos separar os dados nas classes que definimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ato Declaratório Interpretativo SRF nº 15, de ...</td>\n",
       "      <td>Ato Declaratório Interpretativo SRF nº 15, de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ato Declaratório nº 01, de 2013</td>\n",
       "      <td>* Publicado no DOE em 01/02/2013 ATO DECLARA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ato Declaratório nº 02, de 2013</td>\n",
       "      <td>* Publicado no DOE em 01/02/2013 ATO DECLARA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ato Declaratório nº 01, de 1998</td>\n",
       "      <td>ATO DECLARATÓRIO Nº 01/1998 07/01/1998 * Publi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ato Declaratório nº 01, de 1999</td>\n",
       "      <td>ATO DECLARATÓRIO Nº 01/1999 14/01/1999 * Publi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Ato Declaratório Interpretativo SRF nº 15, de ...   \n",
       "1                    Ato Declaratório nº 01, de 2013   \n",
       "2                    Ato Declaratório nº 02, de 2013   \n",
       "3                    Ato Declaratório nº 01, de 1998   \n",
       "4                    Ato Declaratório nº 01, de 1999   \n",
       "\n",
       "                                             content  \n",
       "0  Ato Declaratório Interpretativo SRF nº 15, de ...  \n",
       "1    * Publicado no DOE em 01/02/2013 ATO DECLARA...  \n",
       "2    * Publicado no DOE em 01/02/2013 ATO DECLARA...  \n",
       "3  ATO DECLARATÓRIO Nº 01/1998 07/01/1998 * Publi...  \n",
       "4  ATO DECLARATÓRIO Nº 01/1999 14/01/1999 * Publi...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separando o dataset em classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"ato declaratório\", \"decreto\", \"instrução normativa\", \"lei\", \"norma de execução\", \"nota explicativa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ato declaratório': 672,\n",
       " 'decreto': 1024,\n",
       " 'instrução normativa': 1376,\n",
       " 'lei': 331,\n",
       " 'norma de execução': 117,\n",
       " 'nota explicativa': 84}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_classes = [num_ocorrencias(classe) for classe in classes]\n",
    "{classe: qtd for classe, qtd in zip(classes, dist_classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_classe(titulo):\n",
    "    \"\"\"Retorna a classe adequada para o titulo fornecido\"\"\"\n",
    "    for classe in classes:\n",
    "        if re.match(classe, titulo.lower()):\n",
    "            return classe\n",
    "\n",
    "df['doc_type'] = df['title'].apply(define_classe)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'seaborn' has no attribute 'histplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-49151d6809af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Visualização da distribuição dos documentos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'doc_type'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tipo de documento'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Quantidade'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'seaborn' has no attribute 'histplot'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x504 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualização da distribuição dos documentos\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.histplot(df['doc_type'])\n",
    "plt.xlabel('Tipo de documento')\n",
    "plt.ylabel('Quantidade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instrução normativa    1376\n",
       "decreto                1024\n",
       "ato declaratório        672\n",
       "lei                     331\n",
       "norma de execução       117\n",
       "nota explicativa         84\n",
       "Name: doc_type, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Valores absolutos\n",
    "df['doc_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import das bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separação dos dados em treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['content']\n",
    "y = df['doc_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vetorização dos dados usando CountVectorizer e TFidVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_vec = CountVectorizer()\n",
    "t_vec = TfidfVectorizer()\n",
    "\n",
    "X_train_c = c_vec.fit_transform(X_train)\n",
    "X_train_t = t_vec.fit_transform(X_train)\n",
    "\n",
    "X_test_c = c_vec.transform(X_test)\n",
    "X_test_t = t_vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste com classificadores: NaiveBayes, Decision Tree e Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NaiveBayes com CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "   ato declaratório       1.00      0.96      0.98       243\n",
      "            decreto       0.49      0.98      0.65       340\n",
      "instrução normativa       0.99      0.39      0.56       438\n",
      "                lei       0.86      0.73      0.79       104\n",
      "  norma de execução       1.00      0.29      0.45        38\n",
      "   nota explicativa       0.00      0.00      0.00        27\n",
      "\n",
      "           accuracy                           0.69      1190\n",
      "          macro avg       0.72      0.56      0.57      1190\n",
      "       weighted avg       0.81      0.69      0.67      1190\n",
      "\n",
      "[[234   7   2   0   0   0]\n",
      " [  0 333   0   7   0   0]\n",
      " [  1 265 170   2   0   0]\n",
      " [  0  28   0  76   0   0]\n",
      " [  0  26   0   1  11   0]\n",
      " [  0  25   0   2   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB().fit(X_train_c, y_train)\n",
    "\n",
    "pred = clf.predict(X_test_c)\n",
    "\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naivy Bayes com TFidVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "   ato declaratório       1.00      0.69      0.81       243\n",
      "            decreto       0.76      0.93      0.83       340\n",
      "instrução normativa       0.72      0.99      0.83       438\n",
      "                lei       0.00      0.00      0.00       104\n",
      "  norma de execução       0.00      0.00      0.00        38\n",
      "   nota explicativa       0.00      0.00      0.00        27\n",
      "\n",
      "           accuracy                           0.77      1190\n",
      "          macro avg       0.41      0.43      0.41      1190\n",
      "       weighted avg       0.68      0.77      0.71      1190\n",
      "\n",
      "[[167   0  76   0   0   0]\n",
      " [  0 316  24   0   0   0]\n",
      " [  0   4 434   0   0   0]\n",
      " [  0  91  13   0   0   0]\n",
      " [  0   2  36   0   0   0]\n",
      " [  0   5  22   0   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB().fit(X_train_t, y_train)\n",
    "\n",
    "pred = clf.predict(X_test_t)\n",
    "\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree com CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "   ato declaratório       0.99      1.00      1.00       243\n",
      "            decreto       0.99      0.99      0.99       340\n",
      "instrução normativa       0.99      1.00      0.99       438\n",
      "                lei       0.99      0.99      0.99       104\n",
      "  norma de execução       0.92      0.87      0.89        38\n",
      "   nota explicativa       1.00      0.93      0.96        27\n",
      "\n",
      "           accuracy                           0.99      1190\n",
      "          macro avg       0.98      0.96      0.97      1190\n",
      "       weighted avg       0.99      0.99      0.99      1190\n",
      "\n",
      "[[243   0   0   0   0   0]\n",
      " [  0 336   2   0   2   0]\n",
      " [  1   0 436   0   1   0]\n",
      " [  0   1   0 103   0   0]\n",
      " [  0   2   2   1  33   0]\n",
      " [  1   0   1   0   0  25]]\n"
     ]
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier().fit(X_train_c, y_train)\n",
    "\n",
    "pred = clf.predict(X_test_c)\n",
    "\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree com TFidVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "   ato declaratório       1.00      0.98      0.99       243\n",
      "            decreto       1.00      1.00      1.00       340\n",
      "instrução normativa       0.99      1.00      0.99       438\n",
      "                lei       0.99      1.00      1.00       104\n",
      "  norma de execução       0.95      0.92      0.93        38\n",
      "   nota explicativa       0.89      0.93      0.91        27\n",
      "\n",
      "           accuracy                           0.99      1190\n",
      "          macro avg       0.97      0.97      0.97      1190\n",
      "       weighted avg       0.99      0.99      0.99      1190\n",
      "\n",
      "[[239   0   3   0   0   1]\n",
      " [  0 339   0   0   1   0]\n",
      " [  0   0 436   1   1   0]\n",
      " [  0   0   0 104   0   0]\n",
      " [  0   0   1   0  35   2]\n",
      " [  0   0   2   0   0  25]]\n"
     ]
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier().fit(X_train_t, y_train)\n",
    "\n",
    "pred = clf.predict(X_test_t)\n",
    "\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest com CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "   ato declaratório       1.00      1.00      1.00       243\n",
      "            decreto       0.99      1.00      0.99       340\n",
      "instrução normativa       0.91      1.00      0.95       438\n",
      "                lei       1.00      0.97      0.99       104\n",
      "  norma de execução       1.00      0.32      0.48        38\n",
      "   nota explicativa       0.88      0.26      0.40        27\n",
      "\n",
      "           accuracy                           0.96      1190\n",
      "          macro avg       0.96      0.76      0.80      1190\n",
      "       weighted avg       0.96      0.96      0.95      1190\n",
      "\n",
      "[[242   0   1   0   0   0]\n",
      " [  0 340   0   0   0   0]\n",
      " [  0   2 436   0   0   0]\n",
      " [  0   3   0 101   0   0]\n",
      " [  1   0  24   0  12   1]\n",
      " [  0   0  20   0   0   7]]\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier().fit(X_train_c, y_train)\n",
    "\n",
    "pred = clf.predict(X_test_c)\n",
    "\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest com TFidVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "   ato declaratório       1.00      1.00      1.00       243\n",
      "            decreto       1.00      1.00      1.00       340\n",
      "instrução normativa       0.90      1.00      0.95       438\n",
      "                lei       1.00      0.99      1.00       104\n",
      "  norma de execução       1.00      0.32      0.48        38\n",
      "   nota explicativa       1.00      0.19      0.31        27\n",
      "\n",
      "           accuracy                           0.96      1190\n",
      "          macro avg       0.98      0.75      0.79      1190\n",
      "       weighted avg       0.96      0.96      0.95      1190\n",
      "\n",
      "[[242   0   1   0   0   0]\n",
      " [  0 340   0   0   0   0]\n",
      " [  0   0 438   0   0   0]\n",
      " [  0   1   0 103   0   0]\n",
      " [  1   0  25   0  12   0]\n",
      " [  0   0  22   0   0   5]]\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier().fit(X_train_t, y_train)\n",
    "\n",
    "pred = clf.predict(X_test_t)\n",
    "\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
